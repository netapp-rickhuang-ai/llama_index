{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3d445a1a-961d-4c95-be15-9258b1b81463",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: llama-index-llms-ollama in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (0.5.3)\n",
      "Requirement already satisfied: llama-index-core<0.13.0,>=0.12.4 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-llms-ollama) (0.12.25)\n",
      "Requirement already satisfied: ollama>=0.4.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-llms-ollama) (0.4.7)\n",
      "Requirement already satisfied: PyYAML>=6.0.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy>=1.4.49 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.6 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.11.10)\n",
      "Requirement already satisfied: dataclasses-json in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.6.5)\n",
      "Requirement already satisfied: deprecated>=1.2.9.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.13)\n",
      "Requirement already satisfied: dirtyjson<2.0.0,>=1.0.8 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.8)\n",
      "Requirement already satisfied: eval-type-backport<0.3.0,>=0.2.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.2.2)\n",
      "Requirement already satisfied: filetype<2.0.0,>=1.2.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.0)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2025.3.0)\n",
      "Requirement already satisfied: httpx in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.27.0)\n",
      "Requirement already satisfied: nest-asyncio<2.0.0,>=1.5.8 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.6.0)\n",
      "Requirement already satisfied: networkx>=3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.2.1)\n",
      "Requirement already satisfied: nltk>3.8.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.9.1)\n",
      "Requirement already satisfied: numpy in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.26.4)\n",
      "Requirement already satisfied: pillow>=9.0.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (11.1.0)\n",
      "Requirement already satisfied: pydantic>=2.8.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.10.3)\n",
      "Requirement already satisfied: requests>=2.31.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.2.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (8.2.3)\n",
      "Requirement already satisfied: tiktoken>=0.3.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.9.0)\n",
      "Requirement already satisfied: tqdm<5.0.0,>=4.66.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.67.1)\n",
      "Requirement already satisfied: typing-extensions>=4.5.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.12.2)\n",
      "Requirement already satisfied: typing-inspect>=0.8.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.9.0)\n",
      "Requirement already satisfied: wrapt in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.17.0)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.18.0)\n",
      "Requirement already satisfied: async-timeout<6.0,>=4.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.6->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.0.3)\n",
      "Requirement already satisfied: anyio in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (4.6.2)\n",
      "Requirement already satisfied: certifi in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.2)\n",
      "Requirement already satisfied: idna in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.10)\n",
      "Requirement already satisfied: sniffio in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpcore==1.*->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.14.0)\n",
      "Requirement already satisfied: click in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (8.1.7)\n",
      "Requirement already satisfied: joblib in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from nltk>3.8.1->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2024.11.6)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from pydantic>=2.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.3.2)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests>=2.31.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (2.3.0)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from SQLAlchemy>=1.4.49->SQLAlchemy[asyncio]>=1.4.49->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.1.1)\n",
      "Requirement already satisfied: mypy-extensions>=0.3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from typing-inspect>=0.8.0->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.0.0)\n",
      "Requirement already satisfied: marshmallow<4.0.0,>=3.18.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (3.19.0)\n",
      "Requirement already satisfied: packaging>=17.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from marshmallow<4.0.0,>=3.18.0->dataclasses-json->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (24.2)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from anyio->httpx->llama-index-core<0.13.0,>=0.12.4->llama-index-llms-ollama) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install llama-index-llms-ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8d730560-113d-49c3-9bfd-a331d2e0d6c2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: langchain in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (0.3.12)\n",
      "Requirement already satisfied: PyYAML>=5.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (6.0.2)\n",
      "Requirement already satisfied: SQLAlchemy<3,>=1.4 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (2.0.37)\n",
      "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (3.11.10)\n",
      "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (4.0.3)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.25 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (0.3.25)\n",
      "Requirement already satisfied: langchain-text-splitters<0.4.0,>=0.3.3 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (0.3.3)\n",
      "Requirement already satisfied: langsmith<0.3,>=0.1.17 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (0.1.147)\n",
      "Requirement already satisfied: numpy<2,>=1.22.4 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (1.26.4)\n",
      "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (2.10.3)\n",
      "Requirement already satisfied: requests<3,>=2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (2.32.3)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10,>=8.1.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain) (8.2.3)\n",
      "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.4)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (25.3.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.5.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
      "Requirement already satisfied: propcache>=0.2.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (0.2.0)\n",
      "Requirement already satisfied: yarl<2.0,>=1.17.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.18.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (1.33)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (24.2)\n",
      "Requirement already satisfied: typing-extensions>=4.7 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langchain-core<0.4.0,>=0.3.25->langchain) (4.12.2)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (0.27.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (3.10.14)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from langsmith<0.3,>=0.1.17->langchain) (1.0.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.27.1)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from requests<3,>=2->langchain) (2025.1.31)\n",
      "Requirement already satisfied: greenlet!=0.4.17 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
      "Requirement already satisfied: anyio in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (4.6.2)\n",
      "Requirement already satisfied: httpcore==1.* in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.0.2)\n",
      "Requirement already satisfied: sniffio in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.3.0)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.25->langchain) (2.1)\n",
      "Requirement already satisfied: exceptiongroup>=1.0.2 in ./miniconda3/envs/gpu_env/lib/python3.9/site-packages (from anyio->httpx<1,>=0.23.0->langsmith<0.3,>=0.1.17->langchain) (1.2.0)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install langchain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c6b84fc-3421-433d-85d2-4997e36d1f68",
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.llms.ollama import Ollama"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2c1b2894-72da-4c90-8bd0-1216e756b0f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "American filmmaker, screenwriter, and producer known for \"Panic Room\".\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "\n",
    "# [NOTE] model must be currently serving by Ollama in your env\n",
    "# You can check it in Terminal using command: `ollama ps` to get the NAME of the model\n",
    "\n",
    "model = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "response = model.complete(\"Who is Laurie Voss? write in 10 words\")\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "622375c7-b639-45ed-aea6-8aef9eaf04cd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Floyd Mayweather Jr. is a renowned American professional boxer, known for his undefeated record and lucrative boxing career accomplishments.\n",
      "[EXAMPLE]Total Duration: 3700.125252 ms \n",
      "[EXAMPLE]Load Duration: 22.190161 ms\n",
      "[EXAMPLE]Prompt Eval Duration: 376.0 ms\n",
      "Manny Pacquiao is a Filipino professional boxer, politician, and philanthropist who became the world champion in eight different weight divisions, known for his devastating punching power and inspirational career.\n",
      "\n",
      "[DEBUG] response_1.dict =  <bound method BaseModel.dict of CompletionResponse(text='Manny Pacquiao is a Filipino professional boxer, politician, and philanthropist who became the world champion in eight different weight divisions, known for his devastating punching power and inspirational career.', additional_kwargs={'tool_calls': []}, raw={'model': 'llama3.2:latest', 'created_at': '2025-03-26T17:10:47.58226173Z', 'done': True, 'done_reason': 'stop', 'total_duration': 13404358027, 'load_duration': 35113363, 'prompt_eval_count': 37, 'prompt_eval_duration': 3160000000, 'eval_count': 39, 'eval_duration': 10207000000, 'message': Message(role='assistant', content='Manny Pacquiao is a Filipino professional boxer, politician, and philanthropist who became the world champion in eight different weight divisions, known for his devastating punching power and inspirational career.', images=None, tool_calls=None), 'usage': {'prompt_tokens': 37, 'completion_tokens': 39, 'total_tokens': 76}}, logprobs=None, delta=None)>\n",
      "\n",
      "[DEBUG] response_1.schema =  <bound method BaseModel.schema of <class 'llama_index.core.base.llms.types.CompletionResponse'>>\n",
      "\n",
      "[OUTPUT] Total Duration: 13404.358027 ms\n",
      "\n",
      "[OUTPUT] Load Duration: 35.113363 ms\n",
      "\n",
      "[OUTPUT] Prompt Eval Duration: 3160.0 ms\n",
      "\n",
      "[OUTPUT] Eval Duration: 10207.0 ms\n"
     ]
    }
   ],
   "source": [
    "from llama_index.llms.ollama import Ollama\n",
    "from llama_index.core import Settings\n",
    "from langchain_core.runnables import RunnableLambda, Runnable\n",
    "from datetime import datetime, timezone\n",
    "import time\n",
    "import asyncio\n",
    "\n",
    "\n",
    "model = Ollama(model=\"llama3.2:latest\")\n",
    "\n",
    "response_0 = model.complete(\"Who is Floyd Mayweather? write in 20 words\") # [NOTE] write in 100 words -> response timed out!\n",
    "print(response_0)\n",
    "\n",
    "def format_t(timestamp: float) -> str:\n",
    "    return datetime.fromtimestamp(timestamp, tz=timezone.utc).strftime(\n",
    "        \"%Y-%m-%d %H:%M:%S %Z\"\n",
    "    )\n",
    "\n",
    "def convert_to_milliseconds(duration_ns: int) -> float:\n",
    "    \"\"\"Convert nanoseconds to milliseconds.\"\"\"\n",
    "    return duration_ns / 1_000_000\n",
    "\n",
    "# Example usage of ns to ms conversion \n",
    "total_duration_ns = 3700125252\n",
    "load_duration_ns = 22190161\n",
    "prompt_eval_duration_ns = 376000000\n",
    "\n",
    "print(f\"[EXAMPLE]Total Duration: {convert_to_milliseconds(total_duration_ns)} ms \")\n",
    "print(f\"[EXAMPLE]Load Duration: {convert_to_milliseconds(load_duration_ns)} ms\")\n",
    "print(\n",
    "    f\"[EXAMPLE]Prompt Eval Duration: {convert_to_milliseconds(prompt_eval_duration_ns)} ms\"\n",
    ")\n",
    "\n",
    "\n",
    "async def test_runnable(time_to_sleep: int):\n",
    "    print(f\"Runnable[{time_to_sleep}s]: starts at {format_t(time.time())}\")\n",
    "    await asyncio.sleep(time_to_sleep)\n",
    "    print(f\"Runnable[{time_to_sleep}s]: ends at {format_t(time.time())}\")\n",
    "\n",
    "async def fn_start(run_obj: Runnable):\n",
    "    print(f\"on start callback starts at {format_t(time.time())}\")\n",
    "    await asyncio.sleep(3)\n",
    "    print(f\"on start callback ends at {format_t(time.time())}\")\n",
    "\n",
    "response_1 = model.complete(\n",
    "    \"Who is Manny Pacquiao? write in 30 words\"\n",
    ")  # Pass a dict instead of a single string failed\n",
    "print(response_1)\n",
    "\n",
    "# Extract durations from `raw` in the response\n",
    "print(\"\\n[DEBUG] response_1.dict = \", response_1.dict)\n",
    "print(\"\\n[DEBUG] response_1.schema = \", response_1.schema)\n",
    "\n",
    "# Extract the raw dictionary from the response\n",
    "raw_data = response_1.raw\n",
    "\n",
    "# Extract the durations (in nanoseconds)\n",
    "total_duration_ns = raw_data.get(\"total_duration\", 0)\n",
    "load_duration_ns = raw_data.get(\"load_duration\", 0)\n",
    "prompt_eval_duration_ns = raw_data.get(\"prompt_eval_duration\", 0)\n",
    "eval_duration_ns = raw_data.get(\"eval_duration\", 0)\n",
    "\n",
    "# Convert durations to milliseconds (1 millisecond = 1,000,000 nanoseconds)\n",
    "total_duration_ms = total_duration_ns / 1_000_000\n",
    "load_duration_ms = load_duration_ns / 1_000_000\n",
    "prompt_eval_duration_ms = prompt_eval_duration_ns / 1_000_000\n",
    "eval_duration_ms = eval_duration_ns / 1_000_000\n",
    "\n",
    "# Print the results\n",
    "print(f\"\\n[OUTPUT] Total Duration: {total_duration_ms} ms\")\n",
    "print(f\"\\n[OUTPUT] Load Duration: {load_duration_ms} ms\")\n",
    "print(f\"\\n[OUTPUT] Prompt Eval Duration: {prompt_eval_duration_ms} ms\")\n",
    "print(f\"\\n[OUTPUT] Eval Duration: {eval_duration_ms} ms\")\n",
    "\n",
    "# generation_info = response_1.generations[0][0].generation_info\n",
    "# response_total_duration_ns = generation_info.get(\"total_duration\", 0)\n",
    "# response_load_duration_ns = generation_info.get(\"load_duration\", 0)\n",
    "# response_prompt_eval_duration_ns = generation_info.get(\"prompt_eval_duration\", 0)\n",
    "\n",
    "# # Calculate and display durations in milliseconds\n",
    "# print(f\"[RESPONSE]Total Duration: {convert_to_milliseconds(response_total_duration_ns)} ms\")\n",
    "# print(f\"[RESPONSE]Load Duration: {convert_to_milliseconds(response_load_duration_ns)} ms\")\n",
    "# print(f\"[RESPONSE]Prompt Eval Duration: {convert_to_milliseconds(response_prompt_eval_duration_ns)} ms\")\n",
    "\n",
    "# async def fn_end(run_obj: Runnable):\n",
    "#     print(f\"on end callback starts at {format_t(time.time())}\")\n",
    "#     await asyncio.sleep(2)\n",
    "#     print(f\"on end callback ends at {format_t(time.time())}\")\n",
    "\n",
    "\n",
    "# runnable = RunnableLambda(test_runnable).with_alisteners(\n",
    "#     on_start=fn_start, on_end=fn_end\n",
    "# )\n",
    "\n",
    "# async def concurrent_runs():\n",
    "#     await asyncio.gather(runnable.ainvoke(2), runnable.ainvoke(3))\n",
    "\n",
    "\n",
    "# asyncio.run(concurrent_runs())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8689b1ee-71d8-4139-b6c9-b7b77ea4232d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (GPU)",
   "language": "python",
   "name": "gpu_env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.21"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
